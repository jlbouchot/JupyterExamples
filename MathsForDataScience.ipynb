{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathematical tools for Data Scientists\n",
    "We will review the following topics: \n",
    "-  Probabilities and Markov chains\n",
    "-  Linear algebra, Spectral decomposition, SVD, and Principal Component Analysis\n",
    "-  Optimization, Regularization, and Data approximation\n",
    "\n",
    "These are just very basic overviews and are by no means sufficient. \n",
    "Note also, that you may want to copy these notebooks locally when working on them to avoid conflicts with later versions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of linear algebra\n",
    "Since a lot of problems can be seen as linear (inverse) problems -- e.g. sparse approximation, scientific computing, polynomial approximation/interpolation -- we review here the main ideas, that are mandatory for further processing. \n",
    "\n",
    "### Vector spaces and basis\n",
    "Let $\\mathbb{K}$ ($=\\mathbb{R}$ or $\\mathbb{C}$) be a field and $E$ be an additive commutative group. We say that $E$ is a field over $\\mathbb{K}$ is \n",
    "1. For all $\\lambda \\in \\mathbb{K}$ and $v \\in E$, $\\lambda v \\in E$,\n",
    "2. For all $\\lambda, \\mu \\in \\mathbb{K}$ and $u \\in E$, $(\\lambda + \\mu)v = \\lambda v + \\mu v$,\n",
    "3. For all $\\lambda \\in \\mathbb{K}$ and $u,v \\in E$, $\\lambda (u+v) = \\lambda u + \\lambda v$,\n",
    "4. For all $\\lambda, \\mu \\in \\mathbb{K}$ and $u \\in E$, $(\\lambda \\cdot \\mu)v = \\lambda \\cdot(\\mu \\cdot v)$,\n",
    "5. If $1$ denotes the scalar multiplicative identity, then $1 \\cdot v = v$, for all $v \\in E$. \n",
    "\n",
    "In other words, we can multiply every scalars together, distribute them over the vectors, etc ... \n",
    "\n",
    "Let $\\mathcal{V} = (v_1, \\cdots, v_n)$ ($n$ not necessarily finite) be a collection of $n$ vectors in $E$. We say that $\\mathcal{V}$ is <font color=blue>linearly independent</font> if \n",
    "$$\n",
    "\\sum c_i v_i = 0 \\Leftrightarrow c_i = 0, \\quad \\text{for all } i.\n",
    "$$\n",
    "\n",
    "** Exercise: ** Give an example of vector space that has a family of infinitely many linearly independent vectors. \n",
    "\n",
    "We say that $\\mathcal{V}$ is a <font color=blue>generating set</font> for $E$ if \n",
    "$$\n",
    "\\forall v \\in E, \\exists (c_i)_i \\in \\mathbb{K}^n: v = \\sum c_i \\cdot v_i.\n",
    "$$\n",
    "\n",
    "A family that is both __linearly independent__ and __generating__ is said to be a <font color=blue>basis</font>. The size of the basis is called the <font color=blue> dimension </font> of the vector space. \n",
    "\n",
    "**Exercise: ** Prove that the coefficients representing a given vector on a given basis are unique. \n",
    "\n",
    "** Example: ** Let $\\mathbb{R}_n[x]$ denote the vector space (prove that it is indeed a vector space!) of polynomial of degree at most $n$ over the field of real numbers. It has dimension $n+1$ since every degree $n$ polynomial can be written as $a_0 x^0 + a_1 x^1 + \\cdots + a_n x^n$ and the family $(x^i)_{0 \\leq i \\leq n}$ is linearly independent (prove this!)\n",
    "\n",
    "** Exercise: ** Let $n \\in \\mathbb{N}$ and $0 \\leq i \\leq n$ and let $e_i = \\sum_{j=0}^ix^j \\in E := \\mathbb{R}_n[x]$. Is the family $(e_i)_{i=0}^n$ linearly independent in $E$? A generating set for $E$? A basis for $E$?\n",
    "\n",
    "Having a basis at hand helps us represent vectors the way we are used to: with an ordered set of coefficients!\n",
    "\n",
    "**Example: ** $2+2x+x^2 \\in \\mathbb{R}_2[x]$ corresponds to the representation\n",
    "* $(2,2,1)$ in the basis $\\left( 1,x,x^2  \\right)$ and to \n",
    "* $(0,1,1)$ in the basis $\\left( 1, 1+x, 1+x+x^2 \\right)$\n",
    "\n",
    "**In this example, we see that the choice of basis may have an influence in the representation and hence on further processing of the data!**\n",
    "\n",
    "\n",
    "### Matrices and linear applications\n",
    "Let $E,F$ be vector spaces on $\\mathbb{K}$. We say that a mapping $\\mathcal{A}: E \\to V$ is a linear mapping if $$\\mathcal{A}(u+t\\cdot v) = \\mathcal{A}(u) + t\\cdot \\mathcal{A}(v), \\quad \\text{for all } u,v \\in E \\text{ and } t \\in \\mathbb{K}.$$\n",
    "\n",
    "Given a basis $\\mathcal{B}_E = (e_j)_j$ for $E$, the sole knowledge of $\\mathcal{A}(e_j)$ for all $j$'s is sufficient to completely characterize a linear map. Indeed, noticing that every vector $v \\in E$ can be written (uniquely!) as $v = \\sum c_j e_j$ and together with the linearity property, it follows that \n",
    "$$\n",
    "\\mathcal{A}(v) = \\mathcal{A}(\\sum c_j e_j) = \\sum c_j \\mathcal{A}(e_j) = \\sum c_j u_j, \n",
    "$$\n",
    "where we have defined $u_j = \\mathcal{A}(e_j)$ the image of the basis vectors via through $A$. Since we assumed $F$ to be a vector space, it has a basis $\\mathcal{B}_F = (f_i)$ on which we can represent the images $u_j = \\sum_i a_{ij}f_i$. The matrix $A$ with coefficients $a_{ij}$ characterize the linear map $\\mathcal{A}$ in the basis $\\mathcal{B}_E$ and $\\mathcal{B}_F$. \n",
    "\n",
    "Assuming we are given alternate basis $\\mathcal{B}_E'$ and $\\mathcal{B}_F'$ for $E$ and $F$ respectively, the mapping $\\mathcal{A}$ can be represented with the matrix $A' = QAP^{-1}$ where $Q$ is the transition matrix from basis $\\mathcal{B}_F$ to $\\mathcal{B}_F'$ and $P$ the transition matrix from $\\mathcal{B}_E$ to $\\mathcal{B}_E'$ (i.e. the matrix with the vectors $e_j$ expressed in the basis vectors $e_j'$).\n",
    "\n",
    "_N.B._ Changing the coordinate system will be important later on: PCA _rotate_ teh space according to the variance of the data, SVD _splits_ the domain between the image and the kernel of a (non-squared) matrix. \n",
    "\n",
    "\n",
    "### Eigenvalue decomposition and the spectral theorem\n",
    "\n",
    "\n",
    "### Singular value decomposition and PCA \n",
    "\n",
    "\n",
    "## Optimization and data approximation\n",
    "\n",
    "### Data interpolation as a linear system \n",
    "\n",
    "* Show that everything can be written as a linear system on a basis\n",
    "* Condition number of the linear system\n",
    "* Problem with overfitting\n",
    "\n",
    "### Least squares and data approximation\n",
    "\n",
    "* Overdetermined linear system \n",
    "* Pseudo inverse\n",
    "\n",
    "### Regularization\n",
    "\n",
    "* Prior knowledge and signal model\n",
    "* Tikhonov regularization \n",
    "* Lagrange formulation or optimization with constraints\n",
    "* Cross validation and model selection\n",
    "\n",
    "## Graph-based representations\n",
    "\n",
    "* Coarse definition\n",
    "* Directed, undirected graphs\n",
    "* Spectral clustering: definition of a similarity measure, data points as vertices of a graph, affinity matrix and graph Laplacian, spectral decomposition\n",
    "\n",
    "## Probability theory and Markov property\n",
    "\n",
    "### Basics of probability\n",
    "\n",
    "* Probability distribution\n",
    "* Marginal\n",
    "* Conditional probability\n",
    "* Bayes' theorem\n",
    "\n",
    "### Markov model\n",
    "\n",
    "* What is a Markov chain\n",
    "* Graph representation\n",
    "* Perron Frobernius theorem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
